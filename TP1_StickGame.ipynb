{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apofice2/Test-apprentissage-par-renforcement/blob/main/TP1_StickGame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80826119",
      "metadata": {
        "id": "80826119"
      },
      "source": [
        "<img src=\"./assets/Logo_ESEO_GROUPE.jpg\" alt=\"Tech Logo\" align=\"center\" height=\"400\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7b86566",
      "metadata": {
        "id": "c7b86566"
      },
      "source": [
        "<h1 align=\"center\"; style=\"color:#3333cc;font-size:55px\">Apprentissage par renforcement</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a80ecfac",
      "metadata": {
        "id": "a80ecfac"
      },
      "source": [
        "<h2 align=\"center\"; style=\"color:#0099cc;font-size:30px\">TP1 : D√©veloppez un agent d'apprentissage par renforcement pour jouer au StickGame</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2330aeb",
      "metadata": {
        "id": "b2330aeb"
      },
      "source": [
        "Pour ce TP, vous implementerez un environnement de jeu de Stick Game ainsi qu'un agent capable d'exceller dans cet environnement. Utilisez ce notebook Jupyter qui passe en revue chaque partie du TP pour pr√©senter des preuves et une analyse de vos r√©sultats."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b343a84",
      "metadata": {
        "id": "7b343a84"
      },
      "source": [
        "<h2 style=\"text-align: left; color:#0099cc;font-size: 25px\"><span>üì• <strong>Import des librairies </strong></span></h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35a8a5a2",
      "metadata": {
        "id": "35a8a5a2"
      },
      "outputs": [],
      "source": [
        "from random import randint\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "872f4055",
      "metadata": {
        "id": "872f4055"
      },
      "source": [
        "<h2 style=\"text-align: left; color:#0099cc;font-size: 25px\"><span>üíæ <strong>Les r√®gles du jeu</strong></span></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f0d89f5",
      "metadata": {
        "id": "5f0d89f5"
      },
      "source": [
        "<img src=\"./assets/stickgame.jpg\" alt=\"Tech Logo\" align=\"center\" height=\"400\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d631b71b",
      "metadata": {
        "id": "d631b71b"
      },
      "source": [
        "Le jeu se joue √† deux joueurs. Le joueur qui commence est tir√© au sort. Le jeu commence avec un nombre pr√©d√©fini de batonnets (souvent 12 ou 16). Dans ce TP, nous commencerons le jeu avec 12 batonnets. Chaque joueur enl√®ve au choix 1, 2 ou 3 b√¢tonnets puis c'est au tour de l'adversaire de retirer 1, 2 ou 3 b√¢tonnets. Celui qui enl√®vera le dernier b√¢tonnet sera le perdant."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "011561ed",
      "metadata": {
        "id": "011561ed"
      },
      "source": [
        "<h2 style=\"text-align: left; color:#0099cc;font-size: 25px\"><span>üß†<strong>1. Formalisation du probl√®me d'apprentissage par renforcement</strong></span></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04252cce",
      "metadata": {
        "id": "04252cce"
      },
      "source": [
        "Un probl√®me d'apprentissage par renforcement n√©cessite forc√©ment des interactions entre un agent et un environnement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1f1d5c1",
      "metadata": {
        "id": "f1f1d5c1"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#7a0f43\"><span>‚ùì  Quels sont les principales donn√©es que l'environnement doit fournir √† l'agent ?</span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a13cbac7",
      "metadata": {
        "id": "a13cbac7"
      },
      "outputs": [],
      "source": [
        "#l'√©tat suivant et la r√©compenses suivante"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "274a64af",
      "metadata": {
        "id": "274a64af"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#7a0f43\"><span>‚ùì  En fonction de la r√®gle de jeu du Stick Game √©nonc√©e pr√©c√©demment, quelles valeurs peut-on attribuer √† l'√©tat et au r√©compenses renvoy√©es par l'environnement ? Quel est l'√©tat initial ?</span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ce5c42a",
      "metadata": {
        "id": "6ce5c42a"
      },
      "outputs": [],
      "source": [
        "INITIAL_STATE = 12\n",
        "FAIL_REWARD   =-1\n",
        "GAME_IN_PROGRESS_REWARD = 0\n",
        "WIN_REWARD = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a0d5941",
      "metadata": {
        "id": "1a0d5941"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#7a0f43\"><span>‚ùì  Quelle est la principale donn√©e que l'agent fournit √† l'environnement ?</span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b274aa14",
      "metadata": {
        "id": "b274aa14"
      },
      "outputs": [],
      "source": [
        "#son √©tat actuel et ses actions possibles."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9650fc10",
      "metadata": {
        "id": "9650fc10"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#7a0f43\"><span>‚ùì  En fonction de la r√®gle de jeu du Stick Game √©nonc√©e pr√©c√©demment, quelles sont les actions possibles dans l'espace des actions ? (Exprimez-les sous forme de liste Python)</span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb863ab4",
      "metadata": {
        "id": "fb863ab4"
      },
      "outputs": [],
      "source": [
        "ACTION_SPACE = [1,2,3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29038a2e",
      "metadata": {
        "id": "29038a2e"
      },
      "source": [
        "<h2 style=\"text-align: left; color:#0099cc;font-size: 25px\"><span>üåé <strong>2. Conception de l'environnement</strong></span></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f74ad02",
      "metadata": {
        "id": "2f74ad02"
      },
      "source": [
        "L'environnement sera repr√©sent√© sous forme d'une classe Python ayant des m√©thodes qui lui permettent d'interagir avec l'agent et l'utilisateur du notebook (c'est-√†-dire vous).\n",
        "R√©f√©rez-vous directement aux docstrings de la classe pour comprendre l'utilit√© de chaque attribut et m√©thode de la classe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e19b80c7",
      "metadata": {
        "id": "e19b80c7"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  Ecrivez le code permettant de d√©velopper les m√©thodes de classe <strong>is_finished()</strong>, <strong>reset()</strong>, <strong>display()</strong>, <strong>step()</strong></span></p>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class StickGame(object):\n",
        "    '''\n",
        "    Classe StickGame repr√©sentant l'environnement du jeu des batonnets\n",
        "\n",
        "    '''\n",
        "    def __init__(self, sticks):\n",
        "        # Comme dans toutes les classes, la fonction __init__ est la fonction d'initialisation.\n",
        "        # Cette fonction est appel√©e √† chaque fois qu'un objet de cette classe est instanci√©e\n",
        "        # Au moment de l'initialisation, les attributs de l'objet sont d√©finis\n",
        "\n",
        "        super(StickGame, self).__init__()\n",
        "        # L'attribut original_nb est le nombre de batonnets au d√©but de chaque partie.\n",
        "        # Cet attribut n'est pas modifi√©e au cours du jeu et permet juste de r√©-initialiser le jeu en fin de partie\n",
        "        self.original_sticks = INITIAL_STATE\n",
        "\n",
        "        # L'attribut nb est le nombre de batonnets √† tout moment au cours de la partie.\n",
        "        # Cet attribut est mis √† jour √† chaque fois qu'un joueur joue\n",
        "        self.sticks = INITIAL_STATE\n",
        "\n",
        "    def is_finished(self):\n",
        "\n",
        "        '''\n",
        "        M√©thode permettant de v√©rifier si le jeu est termin√© ou non\n",
        "\n",
        "                Parameters:\n",
        "                        - self : l'instance de classe, permettant d'acc√©der √† tous les attributs de la classe\n",
        "\n",
        "\n",
        "                Returns:\n",
        "                        Bool√©en : True si le jeu est termin√©, False sinon\n",
        "        '''\n",
        "        return self.sticks <= 0\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "        '''\n",
        "        M√©thode permettant de r√©-initialiser le jeu √† la fin de chaque partie\n",
        "\n",
        "                Parameters:\n",
        "                        - self : l'instance de classe, permettant d'acc√©der √† tous les attributs de la classe\n",
        "\n",
        "\n",
        "                Returns:\n",
        "                        self.nb : Le nombre de batonnets au d√©but de chaque partie selon la r√®gle du jeu\n",
        "        '''\n",
        "        self.sticks = self.original_sticks\n",
        "        return self.sticks\n",
        "\n",
        "\n",
        "    def display(self):\n",
        "\n",
        "        '''\n",
        "        M√©thode permettant d'afficher le nombre de batonnets disponibles dans le jeu\n",
        "        apr√®s chaque action de jeu des joueurs\n",
        "\n",
        "                Parameters:\n",
        "                        - self : l'instance de classe, permettant d'acc√©der √† tous les attributs de la classe\n",
        "\n",
        "\n",
        "                Returns:\n",
        "                        Rien : affiche avec print le nombre de batonnets restant disponibles dans le jeu\n",
        "        '''\n",
        "\n",
        "        print (\"| \" * self.sticks)\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        '''\n",
        "        M√©thode permettant de renvoyer √† l'agent apr√®s chaque de sa part le nouvel √©tat de l'environnement\n",
        "        ainsi que la r√©compense obtenue\n",
        "\n",
        "                Parameters:\n",
        "                        - self : l'instance de classe, permettant d'acc√©der √† tous les attributs de la classe\n",
        "                        - action (int) : Nombre entier d√©signant l'action entreprise par l'agent\n",
        "\n",
        "\n",
        "                Returns:\n",
        "                        - current_state (int/None) : Nombre entier ou None (dans le cas o√π le jeu est termin√©)\n",
        "                                                     d√©signant l'√©tat actuel du jeu\n",
        "                        - reward (int) : Nombre entier d√©signant la r√©compense √† fournir √† l'agent\n",
        "                                         en fonction de l'√©tat du jeu\n",
        "\n",
        "        '''\n",
        "        self.sticks = self.sticks - action\n",
        "        if(action not in ACTION_SPACE):\n",
        "            print(ACTION_SPACE)\n",
        "            raise Exception(\"Merci de choisir un nombre entre 1 et 3 svp !\")\n",
        "        else :\n",
        "            if self.sticks <= 0 :\n",
        "                reward = FAIL_REWARD\n",
        "                current_state = None\n",
        "            else :\n",
        "                current_state = self.sticks\n",
        "                reward = GAME_IN_PROGRESS_REWARD\n",
        "\n",
        "            return current_state, reward\n",
        ""
      ],
      "metadata": {
        "id": "S6shIw_WcdYR"
      },
      "id": "S6shIw_WcdYR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2b20d6e",
      "metadata": {
        "id": "d2b20d6e"
      },
      "outputs": [],
      "source": [
        "class StickGame(object):\n",
        "    '''\n",
        "    Classe StickGame repr√©sentant l'environnement du jeu des batonnets\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self, nb):\n",
        "        # Comme dans toutes les classes, la fonction __init__ est la fonction d'initialisation.\n",
        "        # Cette fonction est appel√©e √† chaque fois qu'un objet de cette classe est instanci√©e\n",
        "        # Au moment de l'initialisation, les attributs de l'objet sont d√©finis\n",
        "\n",
        "        super(StickGame, self).__init__()\n",
        "        # L'attribut original_nb est le nombre de batonnets au d√©but de chaque partie.\n",
        "        # Cet attribut n'est pas modifi√©e au cours du jeu et permet juste de r√©-initialiser le jeu en fin de partie\n",
        "        self.original_nb = INITIAL_STATE\n",
        "\n",
        "        # L'attribut nb est le nombre de batonnets √† tout moment au cours de la partie.\n",
        "        # Cet attribut est mis √† jour √† chaque fois qu'un joueur joue\n",
        "        self.nb = INITIAL_STATE\n",
        "\n",
        "    def is_finished(self):\n",
        "\n",
        "        '''\n",
        "        M√©thode permettant de v√©rifier si le jeu est termin√© ou non\n",
        "\n",
        "                Parameters:\n",
        "                        - self : l'instance de classe, permettant d'acc√©der √† tous les attributs de la classe\n",
        "\n",
        "\n",
        "                Returns:\n",
        "                        Bool√©en : True si le jeu est termin√©, False sinon\n",
        "        '''\n",
        "        print(self.nb)\n",
        "        print(type(self.nb))\n",
        "        return self.nb <= 0\n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "        '''\n",
        "        M√©thode permettant de r√©-initialiser le jeu √† la fin de chaque partie\n",
        "\n",
        "                Parameters:\n",
        "                        - self : l'instance de classe, permettant d'acc√©der √† tous les attributs de la classe\n",
        "\n",
        "\n",
        "                Returns:\n",
        "                        self.nb : Le nombre de batonnets au d√©but de chaque partie selon la r√®gle du jeu\n",
        "        '''\n",
        "\n",
        "\n",
        "        self.nb=self.original_nb\n",
        "        return self.nb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def display(self):\n",
        "\n",
        "        '''\n",
        "        M√©thode permettant d'afficher le nombre de batonnets disponibles dans le jeu\n",
        "        apr√®s chaque action de jeu des joueurs\n",
        "\n",
        "                Parameters:\n",
        "                        - self : l'instance de classe, permettant d'acc√©der √† tous les attributs de la classe\n",
        "\n",
        "\n",
        "                Returns:\n",
        "                        Rien : affiche avec print le nombre de batonnets restant disponibles dans le jeu\n",
        "        '''\n",
        "\n",
        "        print (\"| \" * self.nb)\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        '''\n",
        "        M√©thode permettant de renvoyer √† l'agent apr√®s chaque action de sa part le nouvel √©tat de l'environnement\n",
        "        ainsi que la r√©compense obtenue\n",
        "\n",
        "                Parameters:\n",
        "                        - self : l'instance de classe, permettant d'acc√©der √† tous les attributs de la classe\n",
        "                        - action (int) : Nombre entier d√©signant l'action entreprise par l'agent\n",
        "\n",
        "\n",
        "                Returns:\n",
        "                        - current_state (int/None) : Nombre entier ou None (dans le cas o√π le jeu est termin√©)\n",
        "                                                     d√©signant l'√©tat actuel du jeu\n",
        "                        - reward (int) : Nombre entier d√©signant la r√©compense √† fournir √† l'agent\n",
        "                                         en fonction de l'√©tat du jeu\n",
        "\n",
        "        '''\n",
        "        self.nb=self.nb-action\n",
        "        reward=GAME_IN_PROGRESS_REWARD\n",
        "        if self.is_finished()==True :\n",
        "          reward=FAIL_REWARD\n",
        "          self.nb=None\n",
        "        return self.nb , reward\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ef5a8fd",
      "metadata": {
        "id": "1ef5a8fd"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  A pr√©sent, vous pouvez tester votre environnement </span></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43337766",
      "metadata": {
        "id": "43337766"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  Instanciez une partie avec 12 batonnets initiaux </span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8a1b13c",
      "metadata": {
        "id": "e8a1b13c"
      },
      "outputs": [],
      "source": [
        "jeu=StickGame(12)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89cbdb3c",
      "metadata": {
        "id": "89cbdb3c"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  Affichez le jeu </span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8533d1d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8533d1d5",
        "outputId": "5d753f64-ddf0-4780-b583-ec820aceb586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| | | | | | | | | | | | \n"
          ]
        }
      ],
      "source": [
        "jeu.display()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a1c35e0",
      "metadata": {
        "id": "5a1c35e0"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  Faites l'action de retirer 2 batonnets  </span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc4c477",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cc4c477",
        "outputId": "eef2dc66-4177-43ba-b255-dc1d01e9ce20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "<class 'int'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "jeu.step(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83ebc831",
      "metadata": {
        "id": "83ebc831"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  Affichez le jeu </span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4b53f78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4b53f78",
        "outputId": "5e538db9-e033-4654-8122-c97c1c7c7dd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| | | | | | | | | | \n"
          ]
        }
      ],
      "source": [
        "jeu.display()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a776b1bf",
      "metadata": {
        "id": "a776b1bf"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  Faites l'action de retirer 3 batonnets  </span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf6c3ceb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf6c3ceb",
        "outputId": "b9068cee-f2f8-476c-a731-8bb54d968328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "<class 'int'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "jeu.step(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c093ef42",
      "metadata": {
        "id": "c093ef42"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  Affichez le jeu </span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc987c63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc987c63",
        "outputId": "36809c01-98a3-456f-f635-6edf736fb7e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| | | | | | | \n"
          ]
        }
      ],
      "source": [
        "jeu.display()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28966259",
      "metadata": {
        "id": "28966259"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  R√©-initialisez le jeu </span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "246b5206",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "246b5206",
        "outputId": "39d462be-25b8-40e1-fb6a-d9d1a0346b5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "jeu.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b38fdc0",
      "metadata": {
        "id": "6b38fdc0"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  Affichez le jeu </span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47cf7011",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47cf7011",
        "outputId": "331a12eb-94ff-43df-bf2b-04d0860617ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| | | | | | | | | | | | \n"
          ]
        }
      ],
      "source": [
        "jeu.display()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4529151e",
      "metadata": {
        "id": "4529151e"
      },
      "source": [
        "<h2 style=\"text-align: left; color:#0099cc;font-size: 25px\"><span>ü§ñ <strong>3. Conception de l'agent</strong></span></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07fca7a0",
      "metadata": {
        "id": "07fca7a0"
      },
      "source": [
        "L'agent est l'entit√© cl√© d'un probl√®me d'apprentissage par renforcement. C'est lui qui d√©cide des actions √† entreprendre en fonction de l'√©tat de l'environnement.\n",
        "\n",
        "L'agent sera repr√©sent√© sous forme d'une classe Python ayant des m√©thodes qui lui permettent d'interagir avec l'agent et l'utilisateur du notebook.\n",
        "\n",
        "R√©f√©rez-vous directement aux docstrings de la classe pour comprendre l'utilit√© de chaque attribut et m√©thode de la classe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd987f38",
      "metadata": {
        "id": "bd987f38"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  Ecrivez le code permettant de d√©velopper les m√©thodes de classe <strong>reset_stat()</strong>, <strong>greedy_step()</strong>, <strong>play()</strong>, <strong>add_transition()</strong> et <strong>train()</strong> </span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c40daee8",
      "metadata": {
        "id": "c40daee8"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import Self\n",
        "class StickPlayer(object):\n",
        "    '''\n",
        "    Classe StickPlayer repr√©sentant un agent jouant au jeu des batonnets\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self, is_human, size, name, trainable=True):\n",
        "        # Comme dans toutes les classes, la fonction __init__ est la fonction d'initialisation.\n",
        "        # Cette fonction est appel√©e √† chaque fois qu'un objet de cette classe est instanci√©e\n",
        "        # Au moment de l'initialisation, les attributs de l'objet sont d√©finis\n",
        "\n",
        "        super(StickPlayer, self).__init__()\n",
        "\n",
        "        # L'attribut name est une chaine de caract√®re qui repr√©sente le nom du joueur\n",
        "        self.name = name\n",
        "\n",
        "        # L'attribut is_human est un bool√©en qui d√©finit si l'agent √† instancier est un agent humain\n",
        "        # ou un agent artificiel (IA)\n",
        "        self.is_human = is_human\n",
        "\n",
        "        # L'attribut history est une liste Python dans laquelle seront enregistr√©es les trajectoires du jeu\n",
        "        # au cours des diff√©rentes parties jou√©es (√©pisodes)\n",
        "        # Une trajectoire est un ensemble des transitions observ√©es au cours d'une partie (√©pisode)\n",
        "        # Une transition est un tuple (S, A, R, S') o√π\n",
        "        #  - S  : est un l'√©tat courant de l'environnement\n",
        "        #  - A  : est l'action entreprise par l'agent\n",
        "        #  - R  : est la r√©compense donn√©e par l'environnement suite √† l'action A de l'agent\n",
        "        #  - S' : est le nouvel √©tat de l'environnement apr√®s que l'agent ait r√©alis√© l'action A\n",
        "        self.history = []\n",
        "\n",
        "        # L'attribut V est un dictionnaire Python qui repr√©sente la fonction de valeur de l'agent.\n",
        "        # Chaque cl√© du dictionnaire repr√©sente chacun des √©tats de l'environnement\n",
        "        # La valeur associ√©e √† chaque cl√© est la valeur de l'√©tat, c'est-√†-dire √† quel point il est bon\n",
        "        # pour l'agent d'√™tre dans ce √©tat\n",
        "        self.V = {}\n",
        "\n",
        "        # On initialise le dictionnaire avec toutes les valeurs d'√©tat √† 0.\n",
        "        for s in range(1,size+1):\n",
        "            self.V[s] = 0.\n",
        "\n",
        "        # L'attribut win_nb est un nombre entier repr√©sentant le nombre de victoires cumul√©es de l'agent\n",
        "        # sur un ensemble de parties jou√©es.\n",
        "        # Cet attribut sera utilis√© √† des fins de statistiques pour √©valuer l'agent au terme de son entrainement\n",
        "        self.win_nb = 0.\n",
        "\n",
        "        # L'attribut lose_nb est un nombre entier repr√©sentant le nombre de d√©faites cumul√©es de l'agent\n",
        "        # sur un ensemble de parties jou√©es\n",
        "        # Cet attribut sera utilis√© √† des fins de statistiques pour √©valuer l'agent au terme de son entrainement\n",
        "        self.lose_nb = 0.\n",
        "\n",
        "        # L'attribut rewards est une liste Python dans laquelle seront l'ensemble des r√©compenses obtenues\n",
        "        # tout au long des parties jou√©es\n",
        "        # Cet attribut sera utilis√© √† des fins de statistiques pour √©valuer l'agent au terme de son entrainement\n",
        "        self.rewards = []\n",
        "\n",
        "        # L'attribut eps (epsilon) est un nombre r√©el repr√©sentant le coefficient d'exploration initial de l'agent,\n",
        "        # c'est-√†-dire la tendance de l'agent √† entreprendre des actions al√©atoires au lieu d'entreprendre\n",
        "        # des actions permettant d'atteindre l'√©tat avec la plus grande valeur\n",
        "        # Cette valeur diminuera au fur et √† mesure au cours de l'entrainement\n",
        "        self.eps = 0.99\n",
        "\n",
        "        # L'attribut trainable est un bool√©en repr√©sentant si l'agent instanci√© peut √™tre entrain√© ou non.\n",
        "        # Par exemple, si l'agent est un joueur humain, aucun entrainement n'est n√©cessaire.\n",
        "        self.trainable = trainable\n",
        "\n",
        "    def reset_stat(self):\n",
        "\n",
        "        '''\n",
        "        M√©thode permettant de r√©-initialiser les statistiques de victoires et d√©faites de l'agent\n",
        "\n",
        "                Parameters:\n",
        "                        - self : l'instance de classe, permettant d'acc√©der √† tous les attributs de la classe\n",
        "\n",
        "\n",
        "                Returns:\n",
        "                        Rien : r√©-initialise √† 0 les attributs win_nb, lose_nb, rewards.\n",
        "        '''\n",
        "        self.win_nb=0\n",
        "        lose_nb=0\n",
        "        rewards=0\n",
        "\n",
        "\n",
        "\n",
        "    def greedy_step(self, state):\n",
        "\n",
        "        '''\n",
        "        M√©thode permettant de d√©terminer l'action gloutonne (greedy) √† entreprendre en fonction de l'√©tat de l'environnement\n",
        "        Une action gloutonne est une action d'exploitation bas√©e sur la connaissance √† priori de l'agent\n",
        "        C'est-√†-dire atteindre les √©tats avec le plus de valeur.\n",
        "\n",
        "                Parameters:\n",
        "                        - self : l'instance de classe, permettant d'acc√©der √† tous les attributs de la classe\n",
        "                        - state (int) : Nombre entier repr√©sentant l'√©tat courant de l'environnement\n",
        "\n",
        "\n",
        "                Returns:\n",
        "                        - action (int) : Nombre entier repr√©sentant la meilleure action √† entreprendre\n",
        "        '''\n",
        "        actions = ACTION_SPACE\n",
        "        current_worse_value = None\n",
        "        action_index = None\n",
        "\n",
        "        for i in range(0,3):\n",
        "          a = actions[i]\n",
        "\n",
        "          if state - a > 0 and (current_worse_value is None or current_worse_value > self.V[state - a]):\n",
        "            print(\"current_best_value_before : \", current_worse_value)\n",
        "\n",
        "            current_worse_value = self.V[state - a]\n",
        "            action_index = i\n",
        "        return actions[action_index if action_index is not None else np.random.choice(range(len(actions)))]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def play(self, state):\n",
        "\n",
        "        '''\n",
        "        M√©thode qui d√©termine :\n",
        "         - Pour l'agent artificiel : l'action que l'agent doit entreprendre et transmettre √†\n",
        "           l'environnement(action gloutonne ou exploratrice)\n",
        "         - Pour l'agent humain : l'action directement rentr√©e au clavier\n",
        "\n",
        "                Parameters:\n",
        "                        - self : l'instance de classe, permettant d'acc√©der √† tous les attributs de la classe\n",
        "                        - state (int) : Nombre entier repr√©sentant l'√©tat courant de l'environnement\n",
        "\n",
        "\n",
        "                Returns:\n",
        "                        - action (int) : Nombre entier repr√©sentant la meilleure action √† entreprendre\n",
        "        '''\n",
        "        a=1\n",
        "\n",
        "        if self.is_human==True:\n",
        "          while True:\n",
        "\n",
        "            a=int(input())\n",
        "            if a not in [1,2,3]:\n",
        "              print(\"error input incorrect r√©fl√©chie avant d'agir\")\n",
        "            else:\n",
        "              return a\n",
        "              break\n",
        "\n",
        "\n",
        "        else:\n",
        "          randome_valeur=random.uniform(0, 1)\n",
        "          if randome_valeur < self.eps:\n",
        "            a=randint(1,3)\n",
        "          else:\n",
        "\n",
        "            a=self.greedy_step(state)\n",
        "\n",
        "        return a\n",
        "\n",
        "    def add_transition(self, n_tuple):\n",
        "\n",
        "        '''\n",
        "        M√©thode qui rajoute une transition √† (S, A, R, S') √† l'historique  des transitions et\n",
        "        qui rajoute la r√©compense obtenue √† la liste des r√©compenses\n",
        "\n",
        "                Parameters:\n",
        "                        - self : l'instance de classe, permettant d'acc√©der √† tous les attributs de la classe\n",
        "                        - n_tuple (tuple) : Tuple de 4 √©l√©ments (S, A, R, S')\n",
        "\n",
        "\n",
        "                Returns:\n",
        "                        - Rien : transition rajout√©e √† l'historique et r√©compense rajout√©e √† la liste des r√©compenses\n",
        "        '''\n",
        "        self.history.append(n_tuple)\n",
        "        self.rewards.append(n_tuple[2])\n",
        "\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        '''\n",
        "        M√©thode qui r√©alise l'entrainement de l'agent, c'est-√†-dire qui ajuste par it√©ration successives\n",
        "        la fonction des valeurs pour chaque √©tat de l'environnement √† partir des transitions enregistr√©es.\n",
        "\n",
        "                Parameters:\n",
        "                        - self : l'instance de classe, permettant d'acc√©der √† tous les attributs de la classe\n",
        "\n",
        "\n",
        "                Returns:\n",
        "                        - Rien : fonction de valeur mise √† jour it√©rativement, puis historique vid√© √† la fin des it√©rations\n",
        "        '''\n",
        "        if self.trainable==False:\n",
        "          return\".....\"\n",
        "\n",
        "        else:\n",
        "\n",
        "          for i in self.history:\n",
        "            if i[2]== 0:\n",
        "\n",
        "              self.V[i[0]]=self.V[i[0]]+0.02*(self.V[i[3]]-self.V[i[0]])\n",
        "            else:\n",
        "              self.V[i[0]]=self.V[i[0]]+0.02*(self.V[i[3]]-self.V[i[0]])\n",
        "          self.history=[]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KmlubyaNGHIo"
      },
      "id": "KmlubyaNGHIo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "99928b35",
      "metadata": {
        "id": "99928b35"
      },
      "source": [
        "<h2 style=\"text-align: left; color:#0099cc;font-size: 25px\"><span>üéÆ <strong>4. Fonction de jeu</strong></span></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2ee2398",
      "metadata": {
        "id": "a2ee2398"
      },
      "source": [
        "La fonction de jeu permet de faire jouer deux joueurs au StickGame, d'enregistrer les transitions correspondant √† chaque action des joueurs puis d'entrainer les deux agents si besoin. Les agents peuvent √™tre aussi bien des agents artificiels que des agents humains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc978b1c",
      "metadata": {
        "id": "bc978b1c"
      },
      "outputs": [],
      "source": [
        "def play_game(game, p1, p2, train=True):\n",
        "\n",
        "    '''\n",
        "        Fonction qui permet de faire jouer deux agents (humains ou artificiels) au StickGame, d'enregistrer les\n",
        "        transitions dans l'historique des agents, puis de faire l'entrainement des agents si besoin.\n",
        "\n",
        "                Parameters:\n",
        "                        - game (objet StickGame): objet instanci√© de la classe StickGame qui est un environnement de jeu\n",
        "                        - p1 (objet StickPlayer) : objet instanci√© de la classe StickPlayer qui est un agent artificiel ou humain\n",
        "                        - p2 (objet StickPlayer) : objet instanci√© de la classe StickPlayer qui est un agent artificiel ou humain\n",
        "\n",
        "\n",
        "                Returns:\n",
        "                        - Rien\n",
        "    '''\n",
        "\n",
        "    print(\"[INFO] : Starting new game !\")\n",
        "\n",
        "    # Commen√ßons par r√©-initialiser l'environnement de jeu pour s'assurer que l'environnement est dans son √©tat initial\n",
        "    state = game.reset()\n",
        "\n",
        "    # D√©finissons la liste des deux joueurs\n",
        "    # Le joueur en premier dans la liste est celui qui commence √† jouer\n",
        "    players = [p1, p2]\n",
        "\n",
        "    # On utilise la fonction shuffle pour \"m√©langer\" l'ordre des joueurs dans liste et donc choisir al√©atoirement\n",
        "    # le joueur qui commence √† jouer en premier dans la partie\n",
        "    random.shuffle(players)\n",
        "\n",
        "    # Ici, p est le nombre de tours dans une partie.\n",
        "    # On l'initialise √† 0 avant le d√©but d'une partie\n",
        "    p = 0\n",
        "\n",
        "    # On commence ici la boucle de jeu.\n",
        "    # La boucle ne s'interrompt que lorsque le jeu est termin√©, c'est-√†-dire lorsqu'il n'y a plus de batonnets en jeu\n",
        "    while game.is_finished() is False:\n",
        "\n",
        "        # Lorsque le joueur qui joue est un humain, on affiche l'√©tat du jeu\n",
        "        if players[p%2].is_human:\n",
        "            game.display()\n",
        "\n",
        "        # Ici p%2 va varier entre 0 et 1 suivant que p soit pair ou impair\n",
        "        # Par exemple, √† p=0, p%2=0 et donc c'est le premier joueur de la liste (players[0]) qui joue\n",
        "        # A l'it√©ration suivante, √† p=1, p%2=1, et donc c'est le deuxi√®me joueur de la liste (players[1]) qui joue\n",
        "        # Le joueur joue : on r√©cup√®re l'action qu'il choisit d'entreprendre\n",
        "        action = players[p%2].play(state)\n",
        "\n",
        "        # L'action entreprise est transmise √† l'environnement,\n",
        "        # qui en retour communique le nouvel √©tat et la r√©compense obtenue\n",
        "        n_state, reward = game.step(action)\n",
        "\n",
        "        #  Lorsque la r√©compense n'est pas √©gale √† 0, cela veut dire que la partie est termin√©e\n",
        "        # On mets √† jour les statistiques de victoires/d√©faites pour chaque joueur\n",
        "        if (reward != GAME_IN_PROGRESS_REWARD):\n",
        "            print(\"[INFO] : Game Over ! \")\n",
        "\n",
        "            # Le joueur qui a jou√© le dernier coup a perdu\n",
        "            players[p%2].lose_nb += 1. if reward == -1 else 0\n",
        "\n",
        "            # Et l'autre joueur a gagn√©\n",
        "            players[(p+1)%2].win_nb += 1. if reward == -1 else 0\n",
        "            print(\"[INFO] : Player '{}' wins ! \".format(players[(p+1)%2].name))\n",
        "            print(\"===\"*15)\n",
        "\n",
        "\n",
        "        # Add the reversed reward and the new state to the other player\n",
        "\n",
        "        # Rappel : Pour chaque joueur, une transition n'est r√©alis√©e que lorsque l'adversaire a jou√© son coup.\n",
        "        # En d'autres termes, l'√©tat dans lequel il se retrouve suite √† une action n'est calcul√©e qu'apr√®s que l'adversaire ait jou√©\n",
        "        # Cela est n√©cessaire dans l'entrainement de l'agent pour prendre en compte la strat√©gie de l'adversaire\n",
        "        # C'est pour cela qu'√† chaque coup d'un joueur, la derni√®re transition de l'autre joueur (history[-1])\n",
        "        # doit √™tre modifi√©e pour ne pas enregistrer l'√©tat de l'environnement apr√®s son coup, mais apr√®s le coup de l'adversaire\n",
        "        # On en profite √©galement pour modifier la r√©compense qu'il a obtenu apr√®s son dernier coup\n",
        "        # Sa r√©compense est alors l'inverse de la r√©compense du joueur qui joue actuellement\n",
        "        # Lorsque le jeu est en cours, et que la r√©compense obtenue est 0, cela ne change rien (0 x -1 = 0)\n",
        "        # A la fin du jeu, lorsque le joueur dont c'est le tour  a perdu, le joueur qui a jou√© son coup avant lui a gagn√©\n",
        "        # Le joueur dont c'est le tour a la r√©compense -1, et le joueur qui a jou√© au tour pr√©c√©dent √† la r√©compense +1\n",
        "        if p != 0:\n",
        "            s, a, r, sp = players[(p+1)%2].history[-1]\n",
        "            players[(p+1)%2].history[-1] = (s, a, reward * -1, n_state)\n",
        "\n",
        "        # Dans tous les cas, on enregistre partiellement la transition du joueur dont c'est le tour\n",
        "        # L'√©tat futur est None puisque l'√©tat futur ne sera d√©termin√© qu'apr√®s le tour de l'adversaire\n",
        "        players[p%2].add_transition((state, action, reward, None))\n",
        "\n",
        "        # On r√©alise la transition de l'√©tat courant √† l'√©tat suivant\n",
        "        state = n_state\n",
        "\n",
        "        # On incr√©mente le compteur de coups\n",
        "        p += 1\n",
        "\n",
        "        # Et on continue la boucle tant que la partie n'est pas finie\n",
        "\n",
        "    # Si l'argument train est activ√© (train=True), on r√©-entraine les deux agents\n",
        "    if train:\n",
        "        p1.train()\n",
        "        p2.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e657b87",
      "metadata": {
        "id": "8e657b87"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ddcc7fba",
      "metadata": {
        "id": "ddcc7fba"
      },
      "source": [
        "<h2 style=\"text-align: left; color:#0099cc;font-size: 25px\"><span>üë©‚Äçüè´ <strong>5. Entrainement de l'agent</strong></span></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f41d8ae",
      "metadata": {
        "id": "8f41d8ae"
      },
      "source": [
        "L'environnement, l'agent et la fonction de jeu sont maintenant con√ßus.\n",
        "Nous pouvons maintenant passer √† l'entrainement de l'agent afin qu'il puisse apprendre √† exceller au StickGame. L'entrainement consiste √† trouver la fonction de valeur optimale permettant √† l'agent de trouver √† chaque instant l'√©tat qui a le plus de valeur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e203e5",
      "metadata": {
        "id": "29e203e5"
      },
      "outputs": [],
      "source": [
        "N_EPISODES = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27efd9a7",
      "metadata": {
        "id": "27efd9a7"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  Ecrivez le code permettant d'entrainer un agent avec une strat√©gie \"Duel d'agents\" :\n",
        "    <ul style=\"text-align: left; font-size: 16px; color:#0f767a\">\n",
        "    <li>Instancier un environnement de jeu StickGame</li>\n",
        "    <li>Instancier deux agents IA </li>\n",
        "    <li>Faites jouer les deux agents l'un contre l'autre pendant N_EPISODES parties</li>\n",
        "    <li>Chaque 10 parties, diminuez le taux d'exploration de 0.4% sans jamais descendre en dessous de 0.05</li>\n",
        "    <li>A la fin de chaque partie, entrainer les agents avec les transitions enregistr√©es</li>\n",
        "  </ul> </span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7942d87d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "7942d87d",
        "outputId": "f10080a5-aade-400c-a176-e92d351e5e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] : Starting new game !\n",
            "12\n",
            "<class 'int'>\n",
            "9\n",
            "<class 'int'>\n",
            "9\n",
            "<class 'int'>\n",
            "6\n",
            "<class 'int'>\n",
            "6\n",
            "<class 'int'>\n",
            "5\n",
            "<class 'int'>\n",
            "5\n",
            "<class 'int'>\n",
            "3\n",
            "<class 'int'>\n",
            "3\n",
            "<class 'int'>\n",
            "1\n",
            "<class 'int'>\n",
            "1\n",
            "<class 'int'>\n",
            "-2\n",
            "<class 'int'>\n",
            "[INFO] : Game Over ! \n",
            "[INFO] : Player 'david' wins ! \n",
            "=============================================\n",
            "None\n",
            "<class 'NoneType'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-54c0fc9b810f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.004\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-26d784ad6178>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(game, p1, p2, train)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# On commence ici la boucle de jeu.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# La boucle ne s'interrompt que lorsque le jeu est termin√©, c'est-√†-dire lorsqu'il n'y a plus de batonnets en jeu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Lorsque le joueur qui joue est un humain, on affiche l'√©tat du jeu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ab6f8654ab78>\u001b[0m in \u001b[0;36mis_finished\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'NoneType' and 'int'"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "game=StickGame(12)\n",
        "p1=StickPlayer(False,INITIAL_STATE,\"david\",trainable = False)\n",
        "p2=StickPlayer(False,INITIAL_STATE,\"gerard\",trainable = False)\n",
        "#play_game(Game, p1, p2, train=True)\n",
        "\n",
        "for i in range(N_EPISODES):\n",
        "  play_game(game, p1, p2, train=True)\n",
        "  if i%10:\n",
        "    p1.eps=p1.eps-p1.eps*0.004\n",
        "    p2.eps=p2.eps-p2.eps*0.004\n",
        "  play_game(game, p1, p2, train=True)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "287e34e1",
      "metadata": {
        "id": "287e34e1"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  Ecrivez le code permettant de calculer le taux de victoire de chacun des agents, trouvez le meilleur des deux agents puis r√©-initialiser ces statistiques. </span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5263e49",
      "metadata": {
        "id": "f5263e49"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4065ab",
      "metadata": {
        "id": "af4065ab"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "80c24b81",
      "metadata": {
        "id": "80c24b81"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#7a0f43\"><span>‚ùì  Quel est l'√©tat de l'environnement le plus avantageux ? En d'autres termes, dans quel √©tat vaut-il mieux se trouver pour avoir plus de chance de gagner ?</span></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2756cbf6",
      "metadata": {
        "id": "2756cbf6"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#7a0f43\"><span>‚ùì  Quel est l'√©tat de l'environnement le moins avantageux ? En d'autres termes, dans quel √©tat vaut-il mieux √©viter de se trouver pour avoir plus de chance de gagner ?</span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07ce6409",
      "metadata": {
        "id": "07ce6409"
      },
      "outputs": [],
      "source": [
        "# Affichez le fonction de valeur\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75386308",
      "metadata": {
        "id": "75386308"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#7a0f43\"><span>‚ùì  Comparez les fonctions de valeurs des deux agents. Que remarquez-vous ?</span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c9b7572",
      "metadata": {
        "id": "2c9b7572"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2fa83c5b",
      "metadata": {
        "id": "2fa83c5b"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#7a0f43\"><span>‚ùì  A partir de ces observations, quelle condition d√©cide de la victoire de l'un ou l'autre des agents ?</span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a27b919",
      "metadata": {
        "id": "2a27b919"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "55847718",
      "metadata": {
        "id": "55847718"
      },
      "source": [
        "<h2 style=\"text-align: left; color:#0099cc;font-size: 25px\"><span>üß™ <strong>6. Test de l'agent entrain√© contre un agent \"random\"</strong></span></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ff32b66",
      "metadata": {
        "id": "7ff32b66"
      },
      "source": [
        "Dans cette partie, nous faisons jouer notre meilleur agent entrain√© contre un agent artificiel \"random\", c'est-√†-dire un agent qui ne prend que des d√©cisions al√©atoires."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f155d21c",
      "metadata": {
        "id": "f155d21c"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  Ecrivez le code permettant de faire jouer le meileur agent IA contre un agent \"random\" pendant N_EPISODES parties:\n",
        "    <ul style=\"text-align: left; font-size: 16px; color:#0f767a\">\n",
        "    <li>Instancier un agent random</li>\n",
        "    <li>Faire jouer l'agent random contre le meilleur agent artificiel entrain√© pendant N_EPISODES parties</li>\n",
        "  </ul>\n",
        "    </span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a178c047",
      "metadata": {
        "id": "a178c047"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3d75b8b7",
      "metadata": {
        "id": "3d75b8b7"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  Ecrivez le code permettant de calculer le taux de victoire de chacun des deux agents\n",
        "    </span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa977ba1",
      "metadata": {
        "id": "aa977ba1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "00bc2602",
      "metadata": {
        "id": "00bc2602"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#7a0f43\"><span>‚ùì  Il est sureprenant de voir l'agent random gagner tout de m√™me quelques parties. A quoi cela peut-il √™tre d√ª ?</span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc8f16b",
      "metadata": {
        "id": "6bc8f16b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "11ff8a53",
      "metadata": {
        "id": "11ff8a53"
      },
      "source": [
        "<h2 style=\"text-align: left; color:#0099cc;font-size: 25px\"><span>üß™ <strong>6. Test de l'agent entrain√© contre un agent humain</strong></span></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "386a8253",
      "metadata": {
        "id": "386a8253"
      },
      "source": [
        "<p style=\"text-align: left; font-size: 16px; color:#0f767a\"><span>üñ•Ô∏è  Ecrivez le code permettant de faire jouer le meileur agent IA contre un agent humain, c'est-√†-dire vous:\n",
        "    <ul style=\"text-align: left; font-size: 16px; color:#0f767a\">\n",
        "    <li>Instancier un agent humain</li>\n",
        "    <li>Jouer en boucle contre l'agent IA entrain√©</li>\n",
        "  </ul>\n",
        "    </span></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d9dfa61",
      "metadata": {
        "id": "6d9dfa61"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}