{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apofice2/Test-apprentissage-par-renforcement/blob/main/Initiation_A_Gym.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4bd7e42",
      "metadata": {
        "id": "d4bd7e42"
      },
      "source": [
        "<img src=\"./assets/Logo_ESEO_GROUPE.jpg\" alt=\"Tech Logo\" align=\"center\" height=\"400\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8407962",
      "metadata": {
        "id": "c8407962"
      },
      "source": [
        "<h1 align=\"center\"; style=\"color:#3333cc;font-size:55px\">Apprentissage par renforcement</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1eb4be3",
      "metadata": {
        "id": "f1eb4be3"
      },
      "source": [
        "<h2 align=\"center\"; style=\"color:#0099cc;font-size:30px\">Initiez vous √† OpenAI Gym</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2c219ae",
      "metadata": {
        "id": "d2c219ae"
      },
      "source": [
        "Dans ce TP d√©couverte, nous d√©couvrirons les √©l√©ments de base de la librairie Gym d'OpenAI. Nous passerons en revue certains √©l√©ments comme les environnements, les espaces, les wrappers et les environnements vectoris√©s.\n",
        "\n",
        "\n",
        "Pour d√©buter dans l'apprentissage par renforcement, Gym d'OpenAI est ind√©niablement le choix le plus populaire pour mettre en place des environnements pour former vos agents. Dans la recherche en apprentissage par renforcement, un large √©ventail d'environnements qui sont utilis√©s comme r√©f√©rences pour prouver l'efficacit√© de toute nouvelle m√©thodologie de recherche sont impl√©ment√©s dans OpenAI Gym. De plus, OpenAI gym fournit une API simple pour impl√©menter vos propres environnements."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e64981",
      "metadata": {
        "id": "01e64981"
      },
      "source": [
        "<h2 style=\"text-align: left; color:#0099cc;font-size: 25px\"><span>üñ±Ô∏è <strong>Installation de la librairie</strong></span></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db8011f4",
      "metadata": {
        "id": "db8011f4"
      },
      "source": [
        "La premi√®re chose √† faire est de s'assurer que la derni√®re version de gym est install√©e. Sinon, il est possible d'utiliser conda ou pip pour installer Gym. Dans notre cas, nous utiliserons pip."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e264fadf",
      "metadata": {
        "id": "e264fadf"
      },
      "source": [
        "Mais un pr√©alable √† l'installation de gym est l'installation de la librairie **pygame**. Installons-la !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41e6f1a7",
      "metadata": {
        "id": "41e6f1a7"
      },
      "outputs": [],
      "source": [
        "!pip3 install pygame"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de28f702",
      "metadata": {
        "id": "de28f702"
      },
      "source": [
        "Passons maintenant √† l'installation de gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d79f543",
      "metadata": {
        "id": "0d79f543"
      },
      "outputs": [],
      "source": [
        "!pip3 install -U gym==0.19.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27169df0",
      "metadata": {
        "id": "27169df0"
      },
      "source": [
        "<h2 style=\"text-align: left; color:#0099cc;font-size: 25px\"><span>üåé <strong>Les environnements dans Gym</strong></span></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "874f9dd5",
      "metadata": {
        "id": "874f9dd5"
      },
      "source": [
        "Le bloc de base d'OpenAI Gym est la classe Env. Il s'agit d'une classe Python qui impl√©mente essentiellement un simulateur qui ex√©cute l'environnement dans lequel vous souhaitez former votre agent. Open AI Gym est livr√© avec de nombreux environnements, comme celui o√π vous pouvez d√©placer une voiture sur une colline, √©quilibrer un pendule oscillant , obtenir de bons r√©sultats sur les jeux Atari, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84ba9a51",
      "metadata": {
        "id": "84ba9a51"
      },
      "source": [
        "Regardons combien d'environnements sont disponibles dans Gym."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9362b13b",
      "metadata": {
        "id": "9362b13b"
      },
      "outputs": [],
      "source": [
        "from gym import envs\n",
        "print(\"Nombre d'environnements disponibles dans Gym : \", len(envs.registry.all()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "782c0f34",
      "metadata": {
        "id": "782c0f34"
      },
      "source": [
        "Gym vous offre √©galement la possibilit√© de cr√©er des environnements personnalis√©s."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a696d196",
      "metadata": {
        "id": "a696d196"
      },
      "source": [
        "Commen√ßons avec un environnement appel√© MountainCar, o√π l'objectif est de conduire une voiture sur une montagne. La voiture est sur une piste unidimensionnelle, positionn√©e entre deux \"montagnes\". Le but est, en partant de la vall√©e, de monter sur la montagne √† droite. Cependant, le moteur de la voiture n'est pas assez puissant pour escalader la montagne en une seule fois. Par cons√©quent, la seule fa√ßon de r√©ussir est de faire des allers-retours pour cr√©er une dynamique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99039464",
      "metadata": {
        "id": "99039464"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "env = gym.make('MountainCar-v0').env"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2282f738",
      "metadata": {
        "id": "2282f738"
      },
      "source": [
        "<img src=\"./assets/mountain-car-v0.gif\" alt=\"Tech Logo\" align=\"center\" height=\"400\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad63475f",
      "metadata": {
        "id": "ad63475f"
      },
      "source": [
        "La structure de base de l'environnement est d√©crite par les attributs **observation_space** et **action_space** de la classe Env.\n",
        "\n",
        "L'attribut **espace_observation** d√©finit la structure ainsi que les valeurs pour l'observation de l'√©tat de l'environnement. **espace_observation** peut √™tre diff√©rente selon les environnements. La forme la plus courante est une capture d'√©cran du jeu. Il peut aussi y avoir d'autres formes d'observations, comme certaines caract√©ristiques de l'environnement d√©crites sous forme vectorielle.\n",
        "\n",
        "De m√™me, la classe Env fournit √©galement un attribut appel√© **action_space**, qui d√©crit la structure num√©rique des actions pouvant √™tre appliqu√©es √† l'environnement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07c7cd12",
      "metadata": {
        "id": "07c7cd12"
      },
      "outputs": [],
      "source": [
        "# Observation and action space\n",
        "obs_space = env.observation_space\n",
        "action_space = env.action_space\n",
        "print(\"The observation space: {}\".format(obs_space))\n",
        "print(\"The action space: {}\".format(action_space))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a23f2df2",
      "metadata": {
        "id": "a23f2df2"
      },
      "source": [
        "L'espace d'observation pour l'environnement **\"MountainCar\"** est un vecteur de deux nombres repr√©sentant la vitesse et la position. Le point m√©dian entre les deux montagnes est consid√©r√© comme l'origine, la droite √©tant la direction positive et la gauche la direction n√©gative.\n",
        "\n",
        "Remarquez que l'espace d'observation ainsi que l'espace d'action sont repr√©sent√©s par des classes appel√©es **Box** et **Discrete**, respectivement. Il s'agit de l'une des diff√©rentes structures de donn√©es fournies par Gym afin de mettre en ≈ìuvre des espaces d'observation et d'action pour diff√©rents types de sc√©narios (espace d'action discret, espace d'action continue, etc.). Nous approfondirons ces derniers plus loin."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25721de5",
      "metadata": {
        "id": "25721de5"
      },
      "source": [
        "Pour Box et Discrete, l'attribut **n** permet de d√©terminer le nombre d'√©tats possibles dans l'environnement et le nombre d'actions possibles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12b1a6d6",
      "metadata": {
        "id": "12b1a6d6"
      },
      "outputs": [],
      "source": [
        "action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf6684e1",
      "metadata": {
        "id": "cf6684e1"
      },
      "source": [
        "Mais attention, dans des environnements √† espaces d'√©tats continus comme MountainCar, le nombre d'√©tats possibles est infini, donc la commande ci-dessous renverra une erreur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "424a08ba",
      "metadata": {
        "id": "424a08ba"
      },
      "outputs": [],
      "source": [
        "obs_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86480de7",
      "metadata": {
        "id": "86480de7"
      },
      "source": [
        "<h2 style=\"text-align: left; color:#0099cc;font-size: 25px\"><span>üöÄ <strong>Les interactions avec l'environnement</strong></span></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e150e7b9",
      "metadata": {
        "id": "e150e7b9"
      },
      "source": [
        "Dans cette section, nous d√©couvrirons les fonctions de la classe Env qui aident l'agent √† interagir avec l'environnement. Deux de ces fonctions importantes sont :\n",
        "\n",
        "- **reset** : Cette fonction r√©initialise l'environnement √† son √©tat initial, et renvoie l'observation de l'environnement correspondant √† l'√©tat initial.\n",
        "- **step** : cette fonction prend une action en entr√©e et l'applique √† l'environnement, ce qui entra√Æne la transition de l'environnement vers un nouvel √©tat. La fonction reset renvoie quatre choses¬†:\n",
        "    - ***observation*** : L'observation de l'√©tat de l'environnement.\n",
        "    - ***r√©compense***¬†: la r√©compense que vous pouvez obtenir de l'environnement apr√®s avoir ex√©cut√© l'action qui a √©t√© donn√©e en entr√©e de la fonction d'√©tape.\n",
        "    - ***done***¬†: Indique si l'√©pisode a √©t√© termin√©. Si c'est vrai, vous devrez peut-√™tre mettre fin √† la simulation ou r√©initialiser l'environnement pour red√©marrer l'√©pisode.\n",
        "    - ***info***¬†: cela fournit des informations suppl√©mentaires en fonction de l'environnement, telles que le nombre de vies restantes, ou des informations g√©n√©rales pouvant √™tre propices au d√©bogage.\n",
        "\n",
        "Voyons maintenant un exemple qui illustre les concepts √©voqu√©s ci-dessus. On commence d'abord par r√©initialiser l'environnement, puis on inspecte une observation. Nous appliquons ensuite une action et inspectons la nouvelle observation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c5021f5",
      "metadata": {
        "id": "4c5021f5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# reset the environment and see the initial observation\n",
        "obs = env.reset()\n",
        "print(\"L'observation initiale de l'√©tat de l'environnement est : {}\".format(obs))\n",
        "\n",
        "# Choisir une action al√©atoire dans l'espace des actions\n",
        "random_action = env.action_space.sample()\n",
        "\n",
        "# Effectuer l'action choisie et r√©cup√©rer l'observation du nouvel √©tat\n",
        "new_obs, reward, done, info = env.step(random_action)\n",
        "print(\"La nouvelle observation de l'√©tat de l'environnement est : {}\".format(new_obs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad1ce711",
      "metadata": {
        "id": "ad1ce711"
      },
      "source": [
        "Dans le cas de cet environnement, l'observation de l'√©tat de l'environnement n'est pas la capture d'√©cran de la t√¢che en cours d'ex√©cution. Dans de nombreux autres environnements (comme Atari, comme nous le verrons), l'√©tat est une capture d'√©cran du jeu.\n",
        "\n",
        "Dans tous les cas, si vous voulez voir √† quoi ressemble l'environnement dans l'√©tat actuel, vous pouvez utiliser la m√©thode de **render()**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f33a418d",
      "metadata": {
        "id": "f33a418d"
      },
      "outputs": [],
      "source": [
        "env.render(mode=\"human\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30459083",
      "metadata": {
        "id": "30459083"
      },
      "source": [
        "Cela devrait afficher l'environnement dans son √©tat actuel dans une fen√™tre contextuelle. Vous pouvez fermer la fen√™tre √† l'aide de la fonction **\"close\"**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7a0b031",
      "metadata": {
        "id": "b7a0b031"
      },
      "outputs": [],
      "source": [
        "# env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d69aa5b0",
      "metadata": {
        "id": "d69aa5b0"
      },
      "source": [
        "Si vous souhaitez voir une capture d'√©cran du jeu sous forme d'image plut√¥t que sous forme de fen√™tre contextuelle, vous devez d√©finir l'argument mode de la fonction de **render()** sur **rgb_array**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b752b74",
      "metadata": {
        "id": "2b752b74"
      },
      "outputs": [],
      "source": [
        "env_screen = env.render(mode = 'rgb_array')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "print(env_screen.shape)\n",
        "plt.imshow(env_screen)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd363198",
      "metadata": {
        "id": "dd363198"
      },
      "source": [
        "En mettant tout bout √† bout, le code  pour ex√©cuter votre agent dans l'environnement MountainCar ressemblerait √† ce qui suit.\n",
        "\n",
        "Dans notre cas, nous prenons juste des actions al√©atoires, mais vous pouvez bien√©videmment avoir un agent qui fait quelque chose de plus intelligent en fonction de l'observation que vous obtenez (c'est m√™me le but)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7d0db6c",
      "metadata": {
        "id": "b7d0db6c"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Nombre d'√©tapes √† faire ex√©cuter par l'agent\n",
        "num_steps = 1500\n",
        "\n",
        "obs = env.reset()\n",
        "\n",
        "for step in range(num_steps):\n",
        "    # On choisit ici une action al√©atoire\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # On ex√©cute cette action dans l'environnement\n",
        "    obs, reward, done, info = env.step(action)\n",
        "\n",
        "    # On effectue de l'√©tat de l'environnement\n",
        "    env.render()\n",
        "\n",
        "    # On attend un peu sinon la vid√©o finale sera trop rapide √† observer\n",
        "    time.sleep(0.001)\n",
        "\n",
        "    # Si l'√©pisode se termine, on en commence un autre\n",
        "    if done:\n",
        "        env.reset()\n",
        "\n",
        "# AU terme des √©tapes, on ferme l'environnement\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e676ed0",
      "metadata": {
        "id": "8e676ed0"
      },
      "source": [
        "<h2 style=\"text-align: left; color:#0099cc;font-size: 25px\"><span>üåå<strong>Observation de l'espace d'√©tats et d'actions</strong></span></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9f3331e",
      "metadata": {
        "id": "b9f3331e"
      },
      "source": [
        "L'espace des √©tats pour notre environnement √©tait **Box(2,)** et l'espace d'action √©tait **Discrete(2,)**.\n",
        "\n",
        "Mais qu'est-ce que cela signifie r√©ellement? Box et Discrete sont tous deux des types de structures de donn√©es appel√©es \"Spaces\" fournies par Gym pour d√©crire les valeurs l√©gitimes des √©tats et des actions pour les environnements.\n",
        "\n",
        "Toutes ces structures de donn√©es sont d√©riv√©es de la classe de base gym.Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f9ddd6a",
      "metadata": {
        "id": "6f9ddd6a"
      },
      "outputs": [],
      "source": [
        "type(env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d5b6e0a",
      "metadata": {
        "id": "0d5b6e0a"
      },
      "source": [
        "Box(n,) correspond √† l'espace continu √† n dimensions.\n",
        "\n",
        "Dans notre cas n=2, donc l'espace d'observation de notre environnement est un espace 2-D. Bien s√ªr, l'espace est d√©limit√© par des limites sup√©rieures et inf√©rieures qui d√©crivent les valeurs l√©gitimes que peuvent prendre nos √©tats.\n",
        "\n",
        "Nous pouvons le d√©terminer en utilisant les attributs **high** et **low** de l'espace d'√©tat. Celles-ci correspondent respectivement aux positions/vitesses maximales et minimales dans notre environnement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1258ea8e",
      "metadata": {
        "id": "1258ea8e"
      },
      "outputs": [],
      "source": [
        "print(\"Limite sup√©rieure de l'espace des √©tats : \", env.observation_space.high)\n",
        "print(\"Limite inf√©rieure de l'espace des √©tats : \", env.observation_space.low)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e783eddb",
      "metadata": {
        "id": "e783eddb"
      },
      "source": [
        "La l'objet Discret(n) d√©crit un espace discret avec [0.....n-1] valeurs possibles. Dans notre cas, n = 3, ce qui signifie que nos actions peuvent prendre des valeurs de 0, 1 ou 2.\n",
        "\n",
        "Contrairement √† Box, Discrete n'a pas de m√©thode **high** et **low**, car, par d√©finition , l'on sait quel type de valeurs d'actions sont autoris√©s.\n",
        "\n",
        "Par exemple, si vous essayez d'entrer des valeurs d'action non valides (dans notre cas, disons, 4) dans la fonction **step()** de notre environnement, cela conduira √† une erreur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6beca708",
      "metadata": {
        "id": "6beca708"
      },
      "outputs": [],
      "source": [
        "# Fonctionne\n",
        "env.step(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8ee98c5",
      "metadata": {
        "id": "f8ee98c5"
      },
      "outputs": [],
      "source": [
        "# Ne fonctionne pas\n",
        "env.step(4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bd2b1d2",
      "metadata": {
        "id": "8bd2b1d2"
      },
      "source": [
        "Il existe plusieurs autres espaces disponibles pour divers cas d'utilisation, tels que MultiDiscrete, qui permet d'utiliser plusieurs variables discr√®tes pour votre espace d'√©tat et d'action."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11944b0b",
      "metadata": {
        "id": "11944b0b"
      },
      "source": [
        "<h2 style=\"text-align: left; color:#0099cc;font-size: 25px\"><span>üíº<strong>Les Wrappers dans Gym</strong></span></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d878a8b7",
      "metadata": {
        "id": "d878a8b7"
      },
      "source": [
        "La classe **Wrapper** dans OpenAI Gym vous offre la possibilit√© de modifier diff√©rentes parties d'un environnement en fonction de vos besoins.\n",
        "\n",
        "Pourquoi un tel besoin pourrait-il survenir ? Peut-√™tre voulez-vous normaliser votre entr√©e de pixels, ou peut-√™tre voulez-vous modifier vos r√©compenses. Bien que vous puissiez g√©n√©ralement accomplir la m√™me chose en cr√©ant une autre classe qui sous-classe la classe Env de votre environnement, la classe Wrapper nous permet de le faire de mani√®re plus syst√©matique."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8124943f",
      "metadata": {
        "id": "8124943f"
      },
      "source": [
        "Mais avant de commencer, passons √† un environnement plus complexe qui nous aidera vraiment √† appr√©cier l'utilit√© de Wrapper. Cet environnement complexe va √™tre le jeu Atari **Breakout**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "502931aa",
      "metadata": {
        "id": "502931aa"
      },
      "source": [
        "<img src=\"./assets/breakout.gif\" alt=\"Tech Logo\" align=\"center\" height=\"300\" width=\"300\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fdec0ef",
      "metadata": {
        "id": "8fdec0ef"
      },
      "source": [
        "Avant de commencer, nous installons les composants Atari de Gym."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7158236d",
      "metadata": {
        "id": "7158236d"
      },
      "outputs": [],
      "source": [
        "!pip3 install opencv-python\n",
        "!pip3 install gym[atari]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0817897",
      "metadata": {
        "id": "f0817897"
      },
      "source": [
        "Si vous avez une erreur  ***\"AttributeError: le module 'enum' n'a pas d'attribut 'IntFlag'\"***, vous devrez peut-√™tre d√©sinstaller le package enum, puis r√©essayer l'installation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce1f46d6",
      "metadata": {
        "id": "ce1f46d6"
      },
      "outputs": [],
      "source": [
        "# pip3 uninstall -y enum34"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bae4571a",
      "metadata": {
        "id": "bae4571a"
      },
      "source": [
        "Ex√©cutons maintenant l'environnement avec des actions al√©atoires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5913202",
      "metadata": {
        "scrolled": true,
        "id": "c5913202"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
        "\n",
        "print(\"Espace d'√©tats : \", env.observation_space)\n",
        "print(\"Espace d'actions : \", env.action_space)\n",
        "\n",
        "obs = env.reset()\n",
        "\n",
        "for i in range(1000):\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, done, info = env.step(action)\n",
        "    env.render()\n",
        "    time.sleep(0.01)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8f54562",
      "metadata": {
        "id": "b8f54562"
      },
      "source": [
        "Notre espace d'√©tat est un espace continu de dimensions (210, 160, 3) correspondant o√π chaque √©l√©ment est un pixel RGB.\n",
        "Notre espace d'action contient 4 actions discr√®tes :\n",
        "- Gauche,\n",
        "- Droite,\n",
        "- Ne rien faire\n",
        "- Feu\n",
        "\n",
        "Maintenant que notre environnement est charg√©, supposons que nous devions apporter certaines modifications √† l'environnement Atari. C'est une pratique courante en apprentissage par renforcement de construire notre observation en concat√©nant ensemble les k images pr√©c√©dente. Nous devons modifier l'environnement BreakOut de sorte que nos fonctions **reset()** et **step()** renvoient des observations concat√©n√©es.\n",
        "\n",
        "Pour cela, nous d√©finissons une classe de type gym.Wrapper pour surcharger les fonctions reset et return de l'environnement Breakout. La classe Wrapper, comme son nom l'indique, est un wrapper au-dessus d'une classe Env qui modifie certains de ses attributs et fonctions.\n",
        "\n",
        "La fonction **__init__** est d√©finie avec la classe Env pour laquelle le wrapper est √©crit, et le nombre de trames pass√©es √† concat√©ner. Notez que nous devons √©galement red√©finir l'espace d'observation puisque nous utilisons maintenant des cadres concat√©n√©s comme observations. (Nous modifions l'espace d'observation de (210, 160, 3) √† (210, 160, 3 * num_past_frames)\n",
        "\n",
        "Dans la fonction **reset**, pour initialiser l'environnement, puisque nous n'avons aucune observation pr√©c√©dente √† concat√©ner, nous concat√©nons uniquement les observations initiales √† plusieurs reprises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75b403fa",
      "metadata": {
        "id": "75b403fa"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "\n",
        "class ConcatObs(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=((k,) + shp), dtype=env.observation_space.dtype)\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        return np.array(self.frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "229eacaa",
      "metadata": {
        "id": "229eacaa"
      },
      "source": [
        "Maintenant, pour obtenir notre environnement modifi√©, nous encapsulons notre environnement Env dans le wrapper que nous venons de cr√©er."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b2699c3",
      "metadata": {
        "id": "5b2699c3"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
        "wrapped_env = ConcatObs(env, 4)\n",
        "\n",
        "print(\"La nouvelle observation est : \", wrapped_env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51776488",
      "metadata": {
        "id": "51776488"
      },
      "source": [
        "V√©rifions maintenant si les observations sont bien concat√©n√©es ou non."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f983967a",
      "metadata": {
        "id": "f983967a"
      },
      "outputs": [],
      "source": [
        "# R√©initialiser l'environnement\n",
        "obs = wrapped_env.reset()\n",
        "print(\"Dimensions de l'√©tat initiale : \", obs.shape)\n",
        "\n",
        "# Effectuer une action sur l'environnement\n",
        "obs, _, _, _  = wrapped_env.step(2)\n",
        "print(\"Dimensions de l'√©tat apr√®s avoir effectu√© l'action\", obs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dac9960",
      "metadata": {
        "id": "1dac9960"
      },
      "source": [
        "Il existe d'autres types de wrappers dans Gym.\n",
        "\n",
        "Gym fournit √©galement des wrappers sp√©cifiques qui ciblent des √©l√©ments sp√©cifiques de l'environnement, tels que des observations, des r√©compenses et des actions.\n",
        "\n",
        "- **ObservationWrapper** : Il permet d'apporter des modifications √† un √©tat d'un environnement en utilisant la m√©thode **observation** de la classe wrapper.\n",
        "- **RewardWrapper** : Il permet d'apporter des modifications √† une r√©compense en utilisant la m√©thode **reward** de la classe wrapper.\n",
        "- **ActionWrapper**¬†: Il permet d'apporter des modifications √† une action √† l'aide de la m√©thode **action** de la classe wrapper."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1da6186",
      "metadata": {
        "id": "d1da6186"
      },
      "source": [
        "<h2 style=\"text-align: left; color:#0099cc;font-size: 25px\"><span>üßÆ<strong>Les environnements vectoriels dans Gym</strong></span></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8733ebc2",
      "metadata": {
        "id": "8733ebc2"
      },
      "source": [
        "De nombreux algorithmes d'apprentissage par renforcement (comme Asynchronous Actor Critic) utilisent des threads parall√®les, o√π chaque thread ex√©cute une instance de l'environnement pour √† la fois acc√©l√©rer le processus de formation et am√©liorer l'efficacit√©.\n",
        "\n",
        "Pour ce faire, nous allons maintenant utiliser une autre biblioth√®que, √©galement d'OpenAI, appel√©e **baselines**. Cette biblioth√®que fournit des impl√©mentations performantes de nombreux algorithmes d'apprentissage par renforcement standard avec lesquels comparer n'importe quel nouvel algorithme. En plus de ces impl√©mentations, **baselines** fournit √©galement de nombreuses autres fonctionnalit√©s qui permettent de pr√©parer nos environnements conform√©ment √† la mani√®re dont ils ont √©t√© utilis√©s dans les exp√©riences OpenAI.\n",
        "\n",
        "L'une de ces fonctionnalit√©s comprend des wrappers qui vous permettent d'ex√©cuter plusieurs environnements en parall√®le √† l'aide d'un seul appel de fonction. Avant de commencer, nous proc√©dons d'abord √† l'installation de baselines en ex√©cutant les commandes suivantes dans un terminal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb9540bf",
      "metadata": {
        "id": "bb9540bf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/openai/baselines\n",
        "!cd baselines && pip3 install . && cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcf069f3",
      "metadata": {
        "id": "fcf069f3"
      },
      "source": [
        "Vous devrez peut-√™tre red√©marrer votre notebook Jupyter pour que le package install√© soit disponible.\n",
        "\n",
        "Le wrapper qui nous int√©resse ici s'appelle SubProcEnv, qui ex√©cutera tous les environnements dans une m√©thode asynchrone.\n",
        "\n",
        "Nous cr√©ons d'abord une liste d'appels de fonction qui renvoient l'environnement que nous ex√©cutons. Dans le code, une fonction lambda est utilis√©e pour cr√©er une fonction anonyme qui renvoie l'environnement de Gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a08136",
      "metadata": {
        "id": "f9a08136"
      },
      "outputs": [],
      "source": [
        "# Importer les packages necessaires\n",
        "import gym\n",
        "from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
        "\n",
        "# Liste des environnements\n",
        "num_envs = 3\n",
        "envs = [lambda: gym.make(\"BreakoutNoFrameskip-v4\") for i in range(num_envs)]\n",
        "\n",
        "# Cr√©ation de l'environnement vectoriel\n",
        "envs = SubprocVecEnv(envs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc2fae7b",
      "metadata": {
        "id": "dc2fae7b"
      },
      "source": [
        "Cet environnement agit maintenant comme un environnement unique o√π nous pouvons appeler les fonctions **reset** et **step**. Cependant, ces fonctions renvoient maintenant un tableau d'√©tat/actions, plut√¥t qu'un seul √©tat/action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b66bb6ad",
      "metadata": {
        "id": "b66bb6ad"
      },
      "outputs": [],
      "source": [
        "# On se met √† l'√©tat initial\n",
        "init_obs = envs.reset()\n",
        "\n",
        "\n",
        "# On r√©cup√®re une liste d'√©tat correspondant √† des environnements parall√®le\n",
        "print(\"Nombre d'environnements:\", len(init_obs))\n",
        "\n",
        "# On v√©rifie le premier √©tat\n",
        "one_obs = init_obs[0]\n",
        "print(\"Dimensions du premier √©tat\", one_obs.shape)\n",
        "\n",
        "# On pr√©pare une liste d'actions qui sont appliqu√©s √† l'environnement\n",
        "actions = [0, 1, 2]\n",
        "obs = envs.step(actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b102cb07",
      "metadata": {
        "id": "b102cb07"
      },
      "source": [
        "L'appel de la fonction **render** sur les envs vectoris√©s affiche des captures d'√©cran des jeux en mosa√Øque."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eeb30e3",
      "metadata": {
        "id": "5eeb30e3"
      },
      "outputs": [],
      "source": [
        "# Import de libraires\n",
        "import time\n",
        "\n",
        "# Liste des environnements\n",
        "num_envs = 3\n",
        "envs = [lambda: gym.make(\"BreakoutNoFrameskip-v4\") for i in range(num_envs)]\n",
        "\n",
        "# Cr√©ation et rendu de l'environnement vectoris√©\n",
        "envs = SubprocVecEnv(envs)\n",
        "\n",
        "init_obs = envs.reset()\n",
        "\n",
        "for i in range(1000):\n",
        "    actions = [envs.action_space.sample() for i in range(num_envs)]\n",
        "    envs.step(actions)\n",
        "    envs.render()\n",
        "    time.sleep(0.001)\n",
        "\n",
        "envs.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}